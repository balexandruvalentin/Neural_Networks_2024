{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform=lambda x: np.array(x).flatten(), # we convert 28 x 28 into 784 x 1, same as reshaping\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "\n",
    "train_X, train_y = download_mnist(True) # download the train batches\n",
    "test_X, test_y = download_mnist(False) # download the test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# normalize the pixels values from [0, 255] to [0, 1]\n",
    "train_X = np.array(train_X) / 255.0\n",
    "train_y = np.array(train_y)\n",
    "test_X = np.array(test_X) / 255.0\n",
    "test_y = np.array(test_y)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, l2_lambda = 0.001):\n",
    "        # initialize weights (as random values of a normal distribution), biases (all 0, initially) for input state -> hidden state, and hidden state -> output\n",
    "\n",
    "        # input > hidden_state\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01 # has this size because we need an array of size 784 (input_size) weights for each 100 (hidden_size) layers\n",
    "        self.b1 = np.zeros((1, hidden_size)) # bias vector for all hidden layer in total\n",
    "\n",
    "        # hidden_state > output\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01 # same as before but we turn 100 hidden state results into 10 outputs\n",
    "        self.b2 = np.zeros((1, output_size)) # bias vector for all output\n",
    "        self.l2_lambda = l2_lambda # constant used for L2 regularization\n",
    "\n",
    "    # defining math functions, using reLU as an activation function:\n",
    "    # we use this function to introduce non-linearity, since linear transformations alone cannot comprehend the relations between complex data\n",
    "    # another useful trait is that they help avoid \"vanishing gradients\", aka, cases where the gradients get so small it is difficult for the values of the problem to approach\n",
    "    # the desired values and weights and such get updated slower and slower\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float) # cuz if x > 0, reLU(x) = x so derivative is 1, else it is derivative of constant 0 = 0\n",
    "    \n",
    "    # we use softmax to convert values into probabilities\n",
    "    def softmax(self, x):\n",
    "        # we use axis=1 to calculate these values across a particular sample/row (a size 784 vector x), otherwise it's nonsensical to do it on columns\n",
    "        # we use keepdims=True so we get as a result an array of compatible size for other array-wide operations\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) # array-wide (batch) operations, faster than doing a \"for\" loop\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU definition:\n",
    "$$\n",
    "ReLU(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "Derivative of ReLU:\n",
    "$$\n",
    "\\frac{d}{dx}ReLU(x) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } x > 0, \\\\\n",
    "0 & \\text{if } x \\leq 0, \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Softmax definition:\n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "for a value $x_i$ - in our case, $x_i$ is equal to the difference of the $i$-th element of the vector (batch) $X$ and the maximum value from this vector; this is done to prevent overflow and stabilize the data by reducing numbers across the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLP):  # extending the previous class instead of continuing it in the same cell for the sake of readable code and markdown cells inbetween\n",
    "    def forward(self, X):\n",
    "        # forward pass: data flowing from input -> hidden\n",
    "        self.z1 = X.dot(self.W1) + self.b1 # taking inputs and connecting them into hidden layer pre-activation vector z\n",
    "        self.a1 = self.relu(self.z1) # applying activation of the layer, in our case, reLU\n",
    "        \n",
    "        # forward pass: data flowing from hidden -> output\n",
    "        self.z2 = self.a1.dot(self.W2) + self.b2 # taking hidden layer results and connecting them into outputs\n",
    "        output = self.softmax(self.z2) # converting raw numbers into probabilities, aka the output classification values\n",
    "         \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation:\n",
    "\n",
    "Our values go through all the layers to reach the end as output, using the following formulas:\n",
    "$$\n",
    "z_i = W_i \\cdot x + b_i\n",
    "$$\n",
    "for specific values. In our code, we do batch operations, meaning that this is actually applied to the entire vector; as such, we more accurately do\n",
    "$$\n",
    "Z = X \\cdot W + B\n",
    "$$\n",
    "since $X$ has size (N, 784) (for N the amount of samples we are running) and $W$ as a weight matrix has size (784, 100), multiplication can only be done that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLP): \n",
    "    def compute_loss(self, y, output):\n",
    "        # convert to one-hot encoding matrix\n",
    "        # the way this works is basically ensuring each array in the 10 x 10 matrix created defines uniquely a value of y\n",
    "        # for example, if y[3] = 3, then the corresponding row in the matrix will be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "        y_one_hot = np.eye(10)[y]\n",
    "\n",
    "        # cross-entropy loss (average over an entire batch, because we divide by y.shape[0])\n",
    "        # measures the difference between the predicted class probabilities (output) and the true class labels (y_one_hot)\n",
    "        cross_entropy_loss = -np.sum(y_one_hot * np.log(output + 1e-8)) / y.shape[0] # usual entropy formula averaged, with 1e-8 added to prevent log 0 from appearing\n",
    "        \n",
    "        # L2 regularization loss\n",
    "        # we use L2 regularization to prevent overfitting, adding a penalty which increases with bigger weights\n",
    "        # factor of 1/2 is included to simplify the derivative when calculating gradients, apparently good practice\n",
    "        # basically calculated like in the formual below\n",
    "        l2_loss = (self.l2_lambda / 2) * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "        \n",
    "        # total loss is the cross-entropy loss + L2 regularization loss\n",
    "        # loss measures the difference between the expected results and the obtained results, in general\n",
    "        total_loss = cross_entropy_loss + l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy formula:\n",
    "$$\n",
    "\\text{CrossEntropy}(Y_{\\text{one-hot}}, Y_{\\text{output}}) = - \\sum y_{\\text{one-hot}} \\log_2 ( y_{\\text{output}} )\n",
    "$$\n",
    "\n",
    "L2 regularization loss:\n",
    "$$\n",
    "\\text{Loss}_{\\text{L2}} = \\frac{\\lambda}{2} \\left( \\sum_{i} W_{1,i}^2 + \\sum_{j} W_{2,j}^2 \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_1$ and $W_2$ are the weight matrices,\n",
    "- $\\lambda$ is the regularization parameter ('l2_lambda' in code).\n",
    "\n",
    "Total loss:\n",
    "$$\n",
    "\\text{Total Loss} = \\frac{\\text{CrossEntropy}(Y_{\\text{one-hot}}, Y_{\\text{output}})}{N} + \\text{Loss}_{\\text{L2}}\n",
    "$$\n",
    "\n",
    "where $N$ is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # whereas forwardpropagation is used to go from input through hidden layers until the output, to properly train the network, we go BACK and readjust values as needed\n",
    "        # to approach more accurate predictions - this is the basis of backward propagation\n",
    "        # when we say gradient, we refer to the change that must be applied to the parameters of the network in order for higher and higher accuracy\n",
    "\n",
    "        # one-hot encode the labels\n",
    "        y_one_hot = np.eye(10)[y]\n",
    "        \n",
    "        # GRADIENT FOR OUTPUT LAYER IN TERMS OF OBTAINED RESULTS\n",
    "        # difference between predicted values and true values - basically derivative of loss function, can be considered equivalent to an error function\n",
    "        d_z2 = output - y_one_hot\n",
    "\n",
    "        # then we calculate gradient of the weights\n",
    "        # we transpose the activation value array because it has size (N, 100), and we want (100, N) to be able to multiply it with the output layer gradient array of size (N, 10)\n",
    "        d_W2 = self.a1.T.dot(d_z2) / X.shape[0] + self.l2_lambda * self.W2 # gradient of L2 regularization term, formula below\n",
    "        d_b2 = np.sum(d_z2, axis=0, keepdims=True) / X.shape[0] # gradient of biases, calculated by summing the errors across the samples and averaging over the batch\n",
    "        \n",
    "        # GRADIENT FOR HIDDEN LAYER, GOING BACK, USING THE CHAIN RULE (where necessary)\n",
    "        d_a1 = d_z2.dot(self.W2.T) # adjust activation array gradients for the previous layer in terms of the output and weights of the current\n",
    "\n",
    "        # gradient of the loss with respect to pre-activation vector z_1\n",
    "        # since the hidden layer uses ReLU as its activation, we multiply d_a1 element-wise (batch operations, as always) by relu_derivative(self.z1) to apply the chain rule\n",
    "        # this gives the gradient with respect to z1, where ReLU only allows positive values to pass through\n",
    "        d_z1 = d_a1 * self.relu_derivative(self.z1)\n",
    "        \n",
    "        # similar to before, we use the chain rule this time:\n",
    "        # this calculates the gradient for W1 using the input data X and the backpropagated gradient d_z1, dividing by batch size to normalize and adding L2 regularization term\n",
    "        d_W1 = X.T.dot(d_z1) / X.shape[0] + self.l2_lambda * self.W1\n",
    "        d_b1 = np.sum(d_z1, axis=0, keepdims=True) / X.shape[0] # sums the gradient across all examples in the batch for each neuron, normalized by dividing by batch size\n",
    "        \n",
    "        # and finally, we update weights and biases (entire point of backpropagation)\n",
    "        # these adjustments minimize the loss by a small step in the direction opposite to the gradient, moving closer to the loss functionâ€™s minimum, like in the graph\n",
    "        self.W1 -= learning_rate * d_W1\n",
    "        self.b1 -= learning_rate * d_b1\n",
    "        self.W2 -= learning_rate * d_W2\n",
    "        self.b2 -= learning_rate * d_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Calculating gradients of the output:***\n",
    "\n",
    "Gradient of the loss (equivalent to an error function):\n",
    "$$\n",
    "\\mathrm{d}Z_2 = \\hat{Y} - Y = Y_{\\text{output}} - Y_{\\text{one-hot}}\n",
    "$$\n",
    "\n",
    "Gradient of the weights:\n",
    "$$\n",
    "\\mathrm{d}W_2 = \\frac{a_1^T \\cdot \\mathrm{d}Z_2}{N} + \\lambda W_2\n",
    "$$\n",
    "\n",
    "The value added (compared to default backpropagation) is the gradient of the L2 regularization term with respect to the weights:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W_k} \\left( \\frac{\\lambda}{2} \\sum_{i} W_{k,i}^2 \\right) = \\lambda W_k, \\forall k \\in \\{1, 2\\}\n",
    "$$\n",
    "\n",
    "Gradient of the biases:\n",
    "$$\n",
    "\\mathrm{d}b_2 = \\frac{1}{N} \\sum \\mathrm{d}Z_2\n",
    "$$\n",
    "\n",
    "***Going back, updating the gradients of the previous (hidden) layer in terms of the output:***\n",
    "\n",
    "Gradient of the activation vector:\n",
    "$$\n",
    "\\mathrm{d}a_1 = \\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot \\frac{\\partial Z_2}{\\partial a_1} = \\mathrm{d}Z_2 \\cdot W_2^T\n",
    "$$\n",
    "\n",
    "Gradient of the loss:\n",
    "$$\n",
    "\\mathrm{d}Z_1 = \\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial Z_1} = \\mathrm{d}a_1 \\cdot \\frac{\\mathrm{d}}{\\mathrm{d}Z_1}\\text{ReLU}(Z_1)\n",
    "$$\n",
    "\n",
    "Gradient of the weights:\n",
    "$$\n",
    "\\mathrm{d}W_1 = \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1} + \\lambda \\cdot W_1 = \\frac{1}{N} X^T \\cdot \\mathrm{d}Z_1 + \\lambda \\cdot W_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{d}b_1 = \\frac{\\partial L}{\\partial b_1} = \\frac{1}{N} \\sum \\mathrm{d}Z_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training Accuracy: 90.13667% - Validation Accuracy: 90.55000%, Loss: 0.2165\n",
      "Epoch 10/50 - Training Accuracy: 91.85333% - Validation Accuracy: 92.13000%, Loss: 0.1604\n",
      "Epoch 15/50 - Training Accuracy: 92.98167% - Validation Accuracy: 93.04000%, Loss: 0.1403\n",
      "Epoch 20/50 - Training Accuracy: 93.93833% - Validation Accuracy: 93.83000%, Loss: 0.1300\n",
      "Epoch 25/50 - Training Accuracy: 94.62000% - Validation Accuracy: 94.52000%, Loss: 0.1225\n",
      "Epoch 30/50 - Training Accuracy: 95.20000% - Validation Accuracy: 94.98000%, Loss: 0.1161\n",
      "Epoch 35/50 - Training Accuracy: 95.58000% - Validation Accuracy: 95.31000%, Loss: 0.1121\n",
      "Epoch 40/50 - Training Accuracy: 95.90000% - Validation Accuracy: 95.61000%, Loss: 0.1089\n",
      "Epoch 45/50 - Training Accuracy: 96.21167% - Validation Accuracy: 95.88000%, Loss: 0.1066\n",
      "Epoch 50/50 - Training Accuracy: 96.46667% - Validation Accuracy: 96.22000%, Loss: 0.1049\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return np.mean(predictions == labels) * 100\n",
    "\n",
    "def train_mlp(mlp, train_X, train_y, test_X, test_y, epochs, batch_size=64, learning_rate=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        # batch gradient descent\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            # this range is equivalent to for(i = 0; i <= len(train_X); i += 64) in C++\n",
    "\n",
    "            X_batch = train_X[i:i+batch_size] # take a batch\n",
    "            y_batch = train_y[i:i+batch_size] # take a batch\n",
    "            \n",
    "            # forwardpropagation:\n",
    "            output = mlp.forward(X_batch)\n",
    "            \n",
    "            # backpropagation:\n",
    "            mlp.backward(X_batch, y_batch, output, learning_rate)\n",
    "        \n",
    "        # training accuracy:\n",
    "        train_predictions = np.argmax(mlp.forward(train_X), axis=1) # array containing labels with highest probability chosen for current batch of values\n",
    "        train_accuracy = accuracy(train_predictions, train_y) # comparing predictions with actual labels for current batch\n",
    "\n",
    "        # testing accuracy\n",
    "        test_predictions = np.argmax(mlp.forward(test_X), axis=1)\n",
    "        test_accuracy = accuracy(test_predictions, test_y)\n",
    "        loss = mlp.compute_loss(y_batch, output) # compute loss for current batch\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {train_accuracy:.5f}% - Validation Accuracy: {test_accuracy:.5f}%, Loss: {loss:.4f}\")\n",
    "\n",
    "# Initialize and train the MLP\n",
    "mlp = MLP(input_size=784, hidden_size=100, output_size=10, l2_lambda=0.001)\n",
    "train_mlp(mlp, train_X, train_y, test_X, test_y, epochs=50, batch_size=64, learning_rate=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
